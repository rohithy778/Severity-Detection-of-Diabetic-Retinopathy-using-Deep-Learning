{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "gpuClass": "premium"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpPXs4W_IMaZ",
        "outputId": "7213f0c5-227e-4afb-9df7-54a934a97e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hNb6f06NhjeSOtGkods0TMV_GD2Yl-8J\n",
            "To: /content/aptos_data_aug_upload.zip\n",
            "100% 1.08G/1.08G [00:15<00:00, 69.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "! gdown --id 1hNb6f06NhjeSOtGkods0TMV_GD2Yl-8J"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = \"/content/aptos_data_aug_upload.zip\"\n",
        "\n",
        "with ZipFile(file_name,'r') as openZip:\n",
        "  openZip.extractall()\n",
        "  print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh5zKmOGIcGP",
        "outputId": "00b39f10-7c17-4cd8-f873-2398480023ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "pY7iZoUQIkwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
        "                                                          validation_split=0.2,\n",
        "                                                          featurewise_center=True,\n",
        "                                                          featurewise_std_normalization=True)\n",
        "\n",
        "train_data = datagen.flow_from_directory(directory = '/content/aptos_data_aug_upload',\n",
        "                                                  batch_size = 32,\n",
        "                                                  subset = 'training',\n",
        "                                                  target_size = (512, 512),\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  seed = 42)\n",
        "\n",
        "valid_data = datagen.flow_from_directory(directory = '/content/aptos_data_aug_upload',\n",
        "                                                 batch_size = 32,\n",
        "                                                 subset = 'validation',\n",
        "                                                 target_size = (512, 512),\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 seed = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq19e07CIndp",
        "outputId": "899e6c05-9311-409f-8df6-3ec3d070f292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4043 images belonging to 5 classes.\n",
            "Found 1009 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/content/EfficientNet/best_model1.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             mode='max',\n",
        "                             save_best_only=True)\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.1,\n",
        "                              patience=5,\n",
        "                              min_lr=0.00001)"
      ],
      "metadata": {
        "id": "ELjVdEsXWapH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "metadata": {
        "id": "Z9mjf-Y_9RT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_aug_layer = Sequential([\n",
        "    preprocessing.RandomFlip('horizontal'),\n",
        "    preprocessing.RandomRotation(0.3),\n",
        "    #preprocessing.RandomZoom(0.3),\n",
        "    #preprocessing.RandomHeight(0.3),\n",
        "    #preprocessing.RandomWidth(0.3)\n",
        "    #preprocessing.Rescale(1./255)\n",
        "], name = 'data_aug_layer')"
      ],
      "metadata": {
        "id": "tRKnfwDE9J7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.EfficientNetV2B0(include_top = False)\n",
        "base_model.trainable = True\n",
        "\n",
        "for layer in base_model.layers[:-265]:\n",
        "  layer.trainable = False\n",
        "\n",
        "input = Input(shape = (512, 512, 3))\n",
        "x = data_aug_layer(input, training = True)\n",
        "x = base_model(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(10, activation = 'relu')(x)\n",
        "x = Dense(10, activation = 'relu')(x)\n",
        "x = Dense(5, activation = 'softmax')(x)\n",
        "\n",
        "model1 = tf.keras.Model(inputs = input, outputs = x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhLqoeDT9MF0",
        "outputId": "726ff7cb-154d-4326-d6c2-59bd3a901723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "24274472/24274472 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# base_model = tf.keras.applications.VGG19()\n",
        "# base_model.trainable = True"
      ],
      "metadata": {
        "id": "hOGj-n_JaKrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i,layer in enumerate(base_model.layers):\n",
        "#   print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "Jel6BWTETEJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "               optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), # when fine tuning you typically lower the learning rate by some amount\n",
        "               metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "WzK-lNx_9MDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tune for another 15 epochs\n",
        "fine_tune_epochs = 30\n",
        "history1 = model1.fit(train_data,\n",
        "                      epochs = fine_tune_epochs,\n",
        "                      steps_per_epoch=len(train_data),\n",
        "                      validation_data=valid_data,\n",
        "                      validation_steps=len(valid_data),\n",
        "                      callbacks = [checkpoint, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMto6l8C9MAm",
        "outputId": "749f701e-1bfd-4bb7-8b77-4a42ecae2256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1861: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1871: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "127/127 [==============================] - 149s 671ms/step - loss: 1.2380 - accuracy: 0.4954 - val_loss: 1.4628 - val_accuracy: 0.3786 - lr: 1.0000e-04\n",
            "Epoch 2/30\n",
            "127/127 [==============================] - 82s 649ms/step - loss: 1.0261 - accuracy: 0.5981 - val_loss: 1.3167 - val_accuracy: 0.5610 - lr: 1.0000e-04\n",
            "Epoch 3/30\n",
            "127/127 [==============================] - 81s 639ms/step - loss: 0.8695 - accuracy: 0.6315 - val_loss: 1.3932 - val_accuracy: 0.5411 - lr: 1.0000e-04\n",
            "Epoch 4/30\n",
            "127/127 [==============================] - 74s 584ms/step - loss: 0.7527 - accuracy: 0.6923 - val_loss: 1.5285 - val_accuracy: 0.5728 - lr: 1.0000e-04\n",
            "Epoch 5/30\n",
            "127/127 [==============================] - 73s 572ms/step - loss: 0.6265 - accuracy: 0.7984 - val_loss: 1.0466 - val_accuracy: 0.7037 - lr: 1.0000e-04\n",
            "Epoch 6/30\n",
            "127/127 [==============================] - 74s 577ms/step - loss: 0.5541 - accuracy: 0.8140 - val_loss: 0.5145 - val_accuracy: 0.8067 - lr: 1.0000e-04\n",
            "Epoch 7/30\n",
            "127/127 [==============================] - 73s 574ms/step - loss: 0.5035 - accuracy: 0.8279 - val_loss: 0.4251 - val_accuracy: 0.8642 - lr: 1.0000e-04\n",
            "Epoch 8/30\n",
            "127/127 [==============================] - 73s 575ms/step - loss: 0.4643 - accuracy: 0.8415 - val_loss: 0.6125 - val_accuracy: 0.8612 - lr: 1.0000e-04\n",
            "Epoch 9/30\n",
            "127/127 [==============================] - 73s 578ms/step - loss: 0.4265 - accuracy: 0.8516 - val_loss: 0.5514 - val_accuracy: 0.8087 - lr: 1.0000e-04\n",
            "Epoch 10/30\n",
            "127/127 [==============================] - 74s 579ms/step - loss: 0.3978 - accuracy: 0.8575 - val_loss: 0.2630 - val_accuracy: 0.9167 - lr: 1.0000e-04\n",
            "Epoch 11/30\n",
            "127/127 [==============================] - 73s 572ms/step - loss: 0.3703 - accuracy: 0.8719 - val_loss: 0.5152 - val_accuracy: 0.8494 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "127/127 [==============================] - 74s 583ms/step - loss: 0.3495 - accuracy: 0.8778 - val_loss: 0.2644 - val_accuracy: 0.9217 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "127/127 [==============================] - 72s 570ms/step - loss: 0.3460 - accuracy: 0.8748 - val_loss: 1.4050 - val_accuracy: 0.6878 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "127/127 [==============================] - 74s 583ms/step - loss: 0.3408 - accuracy: 0.8783 - val_loss: 0.2564 - val_accuracy: 0.9148 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "127/127 [==============================] - 73s 575ms/step - loss: 0.3123 - accuracy: 0.8899 - val_loss: 0.3058 - val_accuracy: 0.8999 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "127/127 [==============================] - 80s 628ms/step - loss: 0.2830 - accuracy: 0.8981 - val_loss: 0.2503 - val_accuracy: 0.9177 - lr: 1.0000e-04\n",
            "Epoch 17/30\n",
            "127/127 [==============================] - 72s 568ms/step - loss: 0.2847 - accuracy: 0.9008 - val_loss: 0.5531 - val_accuracy: 0.8107 - lr: 1.0000e-04\n",
            "Epoch 18/30\n",
            "127/127 [==============================] - 73s 572ms/step - loss: 0.2523 - accuracy: 0.9090 - val_loss: 0.2389 - val_accuracy: 0.9346 - lr: 1.0000e-04\n",
            "Epoch 19/30\n",
            "127/127 [==============================] - 72s 569ms/step - loss: 0.2519 - accuracy: 0.9124 - val_loss: 0.4714 - val_accuracy: 0.8414 - lr: 1.0000e-04\n",
            "Epoch 20/30\n",
            "127/127 [==============================] - 71s 560ms/step - loss: 0.2408 - accuracy: 0.9186 - val_loss: 1.2292 - val_accuracy: 0.6947 - lr: 1.0000e-04\n",
            "Epoch 21/30\n",
            "127/127 [==============================] - 71s 561ms/step - loss: 0.2107 - accuracy: 0.9256 - val_loss: 0.6988 - val_accuracy: 0.7641 - lr: 1.0000e-04\n",
            "Epoch 22/30\n",
            "127/127 [==============================] - 81s 641ms/step - loss: 0.2145 - accuracy: 0.9238 - val_loss: 0.5240 - val_accuracy: 0.8266 - lr: 1.0000e-04\n",
            "Epoch 23/30\n",
            "127/127 [==============================] - 75s 586ms/step - loss: 0.1998 - accuracy: 0.9256 - val_loss: 0.4769 - val_accuracy: 0.8444 - lr: 1.0000e-04\n",
            "Epoch 24/30\n",
            "127/127 [==============================] - 76s 598ms/step - loss: 0.1725 - accuracy: 0.9389 - val_loss: 0.1826 - val_accuracy: 0.9425 - lr: 1.0000e-05\n",
            "Epoch 25/30\n",
            "127/127 [==============================] - 73s 574ms/step - loss: 0.1553 - accuracy: 0.9478 - val_loss: 0.1975 - val_accuracy: 0.9356 - lr: 1.0000e-05\n",
            "Epoch 26/30\n",
            "127/127 [==============================] - 83s 651ms/step - loss: 0.1537 - accuracy: 0.9473 - val_loss: 0.1715 - val_accuracy: 0.9435 - lr: 1.0000e-05\n",
            "Epoch 27/30\n",
            "127/127 [==============================] - 75s 590ms/step - loss: 0.1477 - accuracy: 0.9513 - val_loss: 0.2239 - val_accuracy: 0.9257 - lr: 1.0000e-05\n",
            "Epoch 28/30\n",
            "127/127 [==============================] - 74s 578ms/step - loss: 0.1487 - accuracy: 0.9495 - val_loss: 0.1755 - val_accuracy: 0.9386 - lr: 1.0000e-05\n",
            "Epoch 29/30\n",
            "127/127 [==============================] - 74s 580ms/step - loss: 0.1455 - accuracy: 0.9478 - val_loss: 0.1785 - val_accuracy: 0.9376 - lr: 1.0000e-05\n",
            "Epoch 30/30\n",
            "127/127 [==============================] - 73s 572ms/step - loss: 0.1363 - accuracy: 0.9545 - val_loss: 0.2011 - val_accuracy: 0.9326 - lr: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv2\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "-dpU7nRe9L9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessed_img(img_path):\n",
        "  img = cv2.imread(img_path)\n",
        "  img_t = cv2.addWeighted(img,4, cv2.GaussianBlur(img , (0,0) , 30) ,-4 ,128)\n",
        "  img = img_t\n",
        "  img = cv2.resize(img, (512, 512))\n",
        "  h, w = img.shape[:2]\n",
        "  cx, cy = w // 2, h // 2\n",
        "\n",
        "  # Define the radius of the circle\n",
        "  r = min(cx, cy)\n",
        "\n",
        "  # Create a mask\n",
        "  mask = np.zeros((h, w), np.uint8)\n",
        "  cv2.circle(mask, (cx, cy), r, 1, -1)\n",
        "\n",
        "  # Apply the mask to the image\n",
        "  result = cv2.bitwise_and(img, img, mask=mask)\n",
        "\n",
        "  # Save the cropped image or display it on screen\n",
        "  return result"
      ],
      "metadata": {
        "id": "AKXC2fRoEKk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = preprocessed_img(\"/content/IDRiD_364.jpg\")\n",
        "test1 = test1/255.\n",
        "output = model1.predict(tf.expand_dims(test1,axis = 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyE0PbBy9L6z",
        "outputId": "bbe6a14e-614e-457e-cedc-acb398320d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Hqwf3FbEVBa",
        "outputId": "72d31b42-5178-4e6b-8897-58c23452ea57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.1281713e-04, 9.8165206e-04, 5.9955705e-02, 3.7471596e-03,\n",
              "        9.3450266e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.save(\"Efficeinet.h5\")"
      ],
      "metadata": {
        "id": "HfXmzFCzEYPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.argmax(output[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuNMjpLgFYlN",
        "outputId": "b549d813-1ed4-423e-e1d0-f1bbe9b46119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=4>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "tf.random.set_seed(4)\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
        "                                                          validation_split=0.2,\n",
        "                                                          featurewise_center=True,\n",
        "                                                          featurewise_std_normalization=True)\n",
        "\n",
        "train_data = datagen.flow_from_directory(directory = '/content/aptos_data_aug_upload',\n",
        "                                                  batch_size = 32,\n",
        "                                                  subset = 'training',\n",
        "                                                  target_size = (512, 512),\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  seed = 42)\n",
        "\n",
        "valid_data = datagen.flow_from_directory(directory = '/content/aptos_data_aug_upload',\n",
        "                                                 batch_size = 32,\n",
        "                                                 subset = 'validation',\n",
        "                                                 target_size = (512, 512),\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 seed = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlKqY84Ba3eU",
        "outputId": "c827d37a-329b-43a5-c46d-2f1cb1e996ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4043 images belonging to 5 classes.\n",
            "Found 1009 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "metadata": {
        "id": "RC4c1PdXa3bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_aug_layer = Sequential([\n",
        "    preprocessing.RandomFlip('horizontal'),\n",
        "    preprocessing.RandomRotation(0.3),\n",
        "    #preprocessing.RandomZoom(0.3),\n",
        "    #preprocessing.RandomHeight(0.3),\n",
        "    #preprocessing.RandomWidth(0.3)\n",
        "    #preprocessing.Rescale(1./255)\n",
        "], name = 'data_aug_layer')"
      ],
      "metadata": {
        "id": "qG8bnJ6da3Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.EfficientNetV2B0(include_top = False)\n",
        "base_model.trainable = True\n",
        "\n",
        "for layer in base_model.layers[:-265]:\n",
        "  layer.trainable = False\n",
        "\n",
        "input = Input(shape = (512, 512, 3))\n",
        "x = data_aug_layer(input, training = True)\n",
        "x = base_model(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(5, activation = 'softmax')(x)\n",
        "\n",
        "model1 = tf.keras.Model(inputs = input, outputs = x)"
      ],
      "metadata": {
        "id": "xxhW1lILa3Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "               optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), # when fine tuning you typically lower the learning rate by some amount\n",
        "               metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "a4b3qVuta3SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tune for another 15 epochs\n",
        "fine_tune_epochs = 30\n",
        "history1 = model1.fit(train_data,\n",
        "                      epochs = fine_tune_epochs,\n",
        "                      steps_per_epoch=len(train_data),\n",
        "                      validation_data=valid_data,\n",
        "                      validation_steps=len(valid_data),\n",
        "                      callbacks = [ reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYzZn7Tva3OU",
        "outputId": "7d8bc16a-869e-411b-abfc-537abdf26b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1861: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py:1871: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "127/127 [==============================] - 117s 440ms/step - loss: 0.9478 - accuracy: 0.6342 - val_loss: 1.6236 - val_accuracy: 0.4202 - lr: 1.0000e-04\n",
            "Epoch 2/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.5877 - accuracy: 0.7757 - val_loss: 2.0138 - val_accuracy: 0.4410 - lr: 1.0000e-04\n",
            "Epoch 3/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.4930 - accuracy: 0.8066 - val_loss: 0.3604 - val_accuracy: 0.8801 - lr: 1.0000e-04\n",
            "Epoch 4/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.4604 - accuracy: 0.8165 - val_loss: 0.5364 - val_accuracy: 0.8167 - lr: 1.0000e-04\n",
            "Epoch 5/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.4013 - accuracy: 0.8429 - val_loss: 0.8217 - val_accuracy: 0.6898 - lr: 1.0000e-04\n",
            "Epoch 6/30\n",
            "127/127 [==============================] - 53s 413ms/step - loss: 0.3564 - accuracy: 0.8630 - val_loss: 0.4826 - val_accuracy: 0.8206 - lr: 1.0000e-04\n",
            "Epoch 7/30\n",
            "127/127 [==============================] - 53s 416ms/step - loss: 0.3430 - accuracy: 0.8679 - val_loss: 0.4239 - val_accuracy: 0.8503 - lr: 1.0000e-04\n",
            "Epoch 8/30\n",
            "127/127 [==============================] - 52s 408ms/step - loss: 0.3319 - accuracy: 0.8709 - val_loss: 0.3003 - val_accuracy: 0.8959 - lr: 1.0000e-04\n",
            "Epoch 9/30\n",
            "127/127 [==============================] - 53s 413ms/step - loss: 0.3023 - accuracy: 0.8830 - val_loss: 0.4213 - val_accuracy: 0.8315 - lr: 1.0000e-04\n",
            "Epoch 10/30\n",
            "127/127 [==============================] - 52s 409ms/step - loss: 0.2819 - accuracy: 0.8931 - val_loss: 0.2906 - val_accuracy: 0.8890 - lr: 1.0000e-04\n",
            "Epoch 11/30\n",
            "127/127 [==============================] - 52s 409ms/step - loss: 0.2635 - accuracy: 0.8993 - val_loss: 0.8953 - val_accuracy: 0.7245 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.2456 - accuracy: 0.9065 - val_loss: 0.6206 - val_accuracy: 0.8196 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "127/127 [==============================] - 52s 409ms/step - loss: 0.2394 - accuracy: 0.9085 - val_loss: 0.3351 - val_accuracy: 0.8831 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.2289 - accuracy: 0.9115 - val_loss: 0.3989 - val_accuracy: 0.8662 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "127/127 [==============================] - 52s 407ms/step - loss: 0.2066 - accuracy: 0.9228 - val_loss: 0.4337 - val_accuracy: 0.8385 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.1696 - accuracy: 0.9399 - val_loss: 0.1364 - val_accuracy: 0.9584 - lr: 1.0000e-05\n",
            "Epoch 17/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.1648 - accuracy: 0.9387 - val_loss: 0.1460 - val_accuracy: 0.9455 - lr: 1.0000e-05\n",
            "Epoch 18/30\n",
            "127/127 [==============================] - 52s 408ms/step - loss: 0.1482 - accuracy: 0.9488 - val_loss: 0.1284 - val_accuracy: 0.9524 - lr: 1.0000e-05\n",
            "Epoch 19/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1498 - accuracy: 0.9456 - val_loss: 0.1282 - val_accuracy: 0.9564 - lr: 1.0000e-05\n",
            "Epoch 20/30\n",
            "127/127 [==============================] - 52s 409ms/step - loss: 0.1420 - accuracy: 0.9518 - val_loss: 0.1389 - val_accuracy: 0.9475 - lr: 1.0000e-05\n",
            "Epoch 21/30\n",
            "127/127 [==============================] - 53s 412ms/step - loss: 0.1364 - accuracy: 0.9493 - val_loss: 0.1270 - val_accuracy: 0.9465 - lr: 1.0000e-05\n",
            "Epoch 22/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.1319 - accuracy: 0.9562 - val_loss: 0.1500 - val_accuracy: 0.9366 - lr: 1.0000e-05\n",
            "Epoch 23/30\n",
            "127/127 [==============================] - 53s 413ms/step - loss: 0.1324 - accuracy: 0.9545 - val_loss: 0.1303 - val_accuracy: 0.9524 - lr: 1.0000e-05\n",
            "Epoch 24/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1319 - accuracy: 0.9518 - val_loss: 0.1455 - val_accuracy: 0.9445 - lr: 1.0000e-05\n",
            "Epoch 25/30\n",
            "127/127 [==============================] - 52s 407ms/step - loss: 0.1263 - accuracy: 0.9547 - val_loss: 0.1411 - val_accuracy: 0.9514 - lr: 1.0000e-05\n",
            "Epoch 26/30\n",
            "127/127 [==============================] - 52s 409ms/step - loss: 0.1320 - accuracy: 0.9530 - val_loss: 0.1566 - val_accuracy: 0.9455 - lr: 1.0000e-05\n",
            "Epoch 27/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.1217 - accuracy: 0.9542 - val_loss: 0.1415 - val_accuracy: 0.9534 - lr: 1.0000e-05\n",
            "Epoch 28/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1242 - accuracy: 0.9582 - val_loss: 0.1603 - val_accuracy: 0.9485 - lr: 1.0000e-05\n",
            "Epoch 29/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1169 - accuracy: 0.9557 - val_loss: 0.1315 - val_accuracy: 0.9564 - lr: 1.0000e-05\n",
            "Epoch 30/30\n",
            "127/127 [==============================] - 52s 407ms/step - loss: 0.1185 - accuracy: 0.9570 - val_loss: 0.1457 - val_accuracy: 0.9504 - lr: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "tf.random.set_seed(40)\n",
        "\n",
        "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
        "                                                          validation_split=0.2,\n",
        "                                                          featurewise_center=True,\n",
        "                                                          featurewise_std_normalization=True)\n",
        "\n",
        "train_data = datagen.flow_from_directory(directory = '/content/aptos_data_aug_upload',\n",
        "                                                  batch_size = 32,\n",
        "                                                  subset = 'training',\n",
        "                                                  target_size = (512, 512),\n",
        "                                                  class_mode = 'categorical',\n",
        "                                                  seed = 42)\n",
        "\n",
        "valid_data = datagen.flow_from_directory(directory = '/content/aptos_data_aug_upload',\n",
        "                                                 batch_size = 32,\n",
        "                                                 subset = 'validation',\n",
        "                                                 target_size = (512, 512),\n",
        "                                                 class_mode = 'categorical',\n",
        "                                                 seed = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYUIAKg7bMse",
        "outputId": "324b1dcf-380f-4205-b256-44c77fa7270f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 4043 images belonging to 5 classes.\n",
            "Found 1009 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.EfficientNetV2B0(include_top = False)\n",
        "base_model.trainable = True\n",
        "\n",
        "for layer in base_model.layers[:-265]:\n",
        "  layer.trainable = False\n",
        "\n",
        "input = Input(shape = (512, 512, 3))\n",
        "x = data_aug_layer(input, training = True)\n",
        "x = base_model(x)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(5, activation = 'softmax')(x)\n",
        "\n",
        "model1 = tf.keras.Model(inputs = input, outputs = x)"
      ],
      "metadata": {
        "id": "JY_Dw5TRhnnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "               optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001), # when fine tuning you typically lower the learning rate by some amount\n",
        "               metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "hU29evjEhsXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tune for another 15 epochs\n",
        "fine_tune_epochs = 30\n",
        "history1 = model1.fit(train_data,\n",
        "                      epochs = fine_tune_epochs,\n",
        "                      steps_per_epoch=len(train_data),\n",
        "                      validation_data=valid_data,\n",
        "                      validation_steps=len(valid_data),\n",
        "                      callbacks = [ reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flfe16CnhuMn",
        "outputId": "54295446-3369-49dc-fc76-40b92ce9e327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "127/127 [==============================] - 99s 423ms/step - loss: 0.9387 - accuracy: 0.6423 - val_loss: 1.2989 - val_accuracy: 0.4727 - lr: 1.0000e-04\n",
            "Epoch 2/30\n",
            "127/127 [==============================] - 52s 407ms/step - loss: 0.5768 - accuracy: 0.7791 - val_loss: 0.7432 - val_accuracy: 0.7364 - lr: 1.0000e-04\n",
            "Epoch 3/30\n",
            "127/127 [==============================] - 53s 413ms/step - loss: 0.4907 - accuracy: 0.8078 - val_loss: 0.8066 - val_accuracy: 0.6779 - lr: 1.0000e-04\n",
            "Epoch 4/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.4367 - accuracy: 0.8316 - val_loss: 0.6485 - val_accuracy: 0.7582 - lr: 1.0000e-04\n",
            "Epoch 5/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.3886 - accuracy: 0.8531 - val_loss: 0.4497 - val_accuracy: 0.8196 - lr: 1.0000e-04\n",
            "Epoch 6/30\n",
            "127/127 [==============================] - 53s 414ms/step - loss: 0.3545 - accuracy: 0.8667 - val_loss: 1.1193 - val_accuracy: 0.6848 - lr: 1.0000e-04\n",
            "Epoch 7/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.3345 - accuracy: 0.8758 - val_loss: 0.3716 - val_accuracy: 0.8692 - lr: 1.0000e-04\n",
            "Epoch 8/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.3145 - accuracy: 0.8778 - val_loss: 0.4433 - val_accuracy: 0.8424 - lr: 1.0000e-04\n",
            "Epoch 9/30\n",
            "127/127 [==============================] - 53s 413ms/step - loss: 0.2969 - accuracy: 0.8830 - val_loss: 0.3299 - val_accuracy: 0.8840 - lr: 1.0000e-04\n",
            "Epoch 10/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.2758 - accuracy: 0.8946 - val_loss: 0.4423 - val_accuracy: 0.8434 - lr: 1.0000e-04\n",
            "Epoch 11/30\n",
            "127/127 [==============================] - 53s 414ms/step - loss: 0.2444 - accuracy: 0.9072 - val_loss: 0.2544 - val_accuracy: 0.9058 - lr: 1.0000e-04\n",
            "Epoch 12/30\n",
            "127/127 [==============================] - 53s 418ms/step - loss: 0.2410 - accuracy: 0.9117 - val_loss: 0.2397 - val_accuracy: 0.9088 - lr: 1.0000e-04\n",
            "Epoch 13/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.2299 - accuracy: 0.9127 - val_loss: 0.7090 - val_accuracy: 0.6987 - lr: 1.0000e-04\n",
            "Epoch 14/30\n",
            "127/127 [==============================] - 53s 414ms/step - loss: 0.2139 - accuracy: 0.9221 - val_loss: 0.4469 - val_accuracy: 0.8186 - lr: 1.0000e-04\n",
            "Epoch 15/30\n",
            "127/127 [==============================] - 53s 419ms/step - loss: 0.2008 - accuracy: 0.9246 - val_loss: 0.5092 - val_accuracy: 0.8216 - lr: 1.0000e-04\n",
            "Epoch 16/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.1767 - accuracy: 0.9345 - val_loss: 0.4398 - val_accuracy: 0.8295 - lr: 1.0000e-04\n",
            "Epoch 17/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.1775 - accuracy: 0.9362 - val_loss: 0.4257 - val_accuracy: 0.8494 - lr: 1.0000e-04\n",
            "Epoch 18/30\n",
            "127/127 [==============================] - 52s 409ms/step - loss: 0.1417 - accuracy: 0.9488 - val_loss: 0.1447 - val_accuracy: 0.9524 - lr: 1.0000e-05\n",
            "Epoch 19/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.1404 - accuracy: 0.9508 - val_loss: 0.1560 - val_accuracy: 0.9475 - lr: 1.0000e-05\n",
            "Epoch 20/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1315 - accuracy: 0.9537 - val_loss: 0.1525 - val_accuracy: 0.9435 - lr: 1.0000e-05\n",
            "Epoch 21/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.1254 - accuracy: 0.9523 - val_loss: 0.1467 - val_accuracy: 0.9465 - lr: 1.0000e-05\n",
            "Epoch 22/30\n",
            "127/127 [==============================] - 52s 412ms/step - loss: 0.1136 - accuracy: 0.9584 - val_loss: 0.1484 - val_accuracy: 0.9504 - lr: 1.0000e-05\n",
            "Epoch 23/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1197 - accuracy: 0.9570 - val_loss: 0.1497 - val_accuracy: 0.9485 - lr: 1.0000e-05\n",
            "Epoch 24/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1185 - accuracy: 0.9540 - val_loss: 0.1496 - val_accuracy: 0.9524 - lr: 1.0000e-05\n",
            "Epoch 25/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1107 - accuracy: 0.9602 - val_loss: 0.1539 - val_accuracy: 0.9504 - lr: 1.0000e-05\n",
            "Epoch 26/30\n",
            "127/127 [==============================] - 52s 411ms/step - loss: 0.1063 - accuracy: 0.9624 - val_loss: 0.1444 - val_accuracy: 0.9485 - lr: 1.0000e-05\n",
            "Epoch 27/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1038 - accuracy: 0.9636 - val_loss: 0.1565 - val_accuracy: 0.9465 - lr: 1.0000e-05\n",
            "Epoch 28/30\n",
            "127/127 [==============================] - 52s 410ms/step - loss: 0.1014 - accuracy: 0.9629 - val_loss: 0.1520 - val_accuracy: 0.9475 - lr: 1.0000e-05\n",
            "Epoch 29/30\n",
            "127/127 [==============================] - 53s 415ms/step - loss: 0.1063 - accuracy: 0.9659 - val_loss: 0.1814 - val_accuracy: 0.9326 - lr: 1.0000e-05\n",
            "Epoch 30/30\n",
            "127/127 [==============================] - 53s 416ms/step - loss: 0.0993 - accuracy: 0.9629 - val_loss: 0.1789 - val_accuracy: 0.9366 - lr: 1.0000e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n"
      ],
      "metadata": {
        "id": "qmpWLZpQWcoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.VGG16(include_top = False)"
      ],
      "metadata": {
        "id": "ULfUWdkwhvCI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d2d5211-63d9-48f2-ec1c-993f1920f991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in base_model.layers:\n",
        "  print(i.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vb0p3rQWa5F",
        "outputId": "666e0c2e-3b08-46d8-8956-0392f98be78e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "6HHc-0iwW4fM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import InceptionV3"
      ],
      "metadata": {
        "id": "CqSVUj8z3YGI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}